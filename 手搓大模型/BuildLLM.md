> Build a Large Language Model (From Scratch)

## 第二章：处理文本数据
### 不懂的词
- 神经元之间的连接权重和偏置在训练过程中不断更新。
- 对于那些熟悉独热编码的人来说，上述嵌入层方法本质上只是实现独热编码后再进行矩阵乘法的一种更高效的方式
#### 连续向量
- 向量值是连续的：向量的每个元素都是一个实数值（如0.2, -0.5），而不是离散的整数0或1。
    - 这个ok
- 语义相近的单词（如“猫”和“狗”）它们的向量在空间中的**距离会很近**（余弦相似度高）。vector(“国王”) - vector(“男人”) + vector(“女人”) ≈ vector(“女王”)
    - 这个还没体会
### 不懂的逻辑
- 嵌入层是随机生成的
```
torch.manual_seed(123)
// 行（词表大小），列（维度）
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
// 取第3行（从0开始）
embedding_layer(torch.tensor([3]))
// token_id -> input_ids行的嵌入向量
// 但embedding_layer是随机的，所以这里看不到token之间的关系？也就是说意思相近的token，嵌入向量不一定近
embedding_layer(input_ids)
```
### 背景知识
- 对计算中向量和张量不熟悉的读者，可以在附录 A 的 A2.2 节中了解更多关于张量的内容。
- 如果你不熟悉神经网络是如何通过反向传播进行训练的，请参阅附录 A 中的 A.4 节，《自动微分简易教程》。
### 其它
- RoPE是一种位置编码，cs336作业里需要，但是这本书里没有讲
## 第三章：实现注意力机制
### 记录
- softmax函数可以实现注意力权重的归一化，使它们的总和为1；softmax函数确保注意力权重始终为正值
    - 它的作用是将一个任意的实数向量转换为一个概率分布，且所有元素的概率之和为 1。
    - Softmax适用于多类别分类问题。
    - Sigmoid：将输入信号转换为0到1之间的概率值，常用于二分类问题。
    - ReLU：将输入信号转换为0到正无穷之间的值，常用于多分类问题。
- 注意力机制最后变成了矩阵运算（过程没太看懂，感觉需要手推一下）
### 问题（需要实验）：
- 二者的区别？
```
torch.empty(inputs.shape[0])
torch.zeros(query.shape)
``` 
### 不懂的词
- QKV：中文译者提到了，但没太理解，且这本书里没有讲 -> 后面有讲 
### 不懂的逻辑
- 与之前的不同之处在于，现在我们通过将注意力得分除以keys嵌入维度的==平方根==来进行缩放（所以这里softmax之后的和不为1）。通过嵌入维度的平方根进行缩放，正是自注意力机制被称为“缩放点积注意力”的原因。
### 背景知识
- 点积和叉积：点积应该是夹角的cosin值（正为锐角，1为相同）；叉积生成一个同时垂直于原始两个向量的法向量，叉积的模长（大小）等于二者所构成的平行四边形的面积
- .T：转置
- view/reshape：重塑，可以实现转置，也可以实现维度合并，维度合并并不会改变矩阵上的值
    - view的第一个参数是-1应该怎么理解？会根据张量的总元素个数和你指定的其他维度，自动推断出-1所代表的位置应该是多少。
- transpose/permute：调整维度顺序，也可以实现转置，调整维度顺序并不会改变矩阵上的值（deepseek举了一个RGB薄膜的例子比较贴切）
    - transpose的维度数量比原张量少怎么理解？
- einsum（torch和eniops中都有）和rearrange：问了deepseek举了一些例子，可能有点懂了，但还需要看具体的case，主要是没搞明白有什么用，以及有些操作不明白物理意义（https://einops.rocks/1-einops-basics/）
- N(μ=0,σ2=1) 表示均值为 0、方差为 1 的正态分布（Normal Distribution），也被称为 “标准正态分布（Standard Normal Distribution）”。
### 其它
